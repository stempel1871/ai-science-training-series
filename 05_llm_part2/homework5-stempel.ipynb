{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30876320-3f1e-4ed7-9a43-2c55ab405150",
   "metadata": {},
   "source": [
    "Sam Tempel\n",
    "\n",
    "ALCFAITS Session 5: Homework\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c46546-15b1-4a0c-a3fb-abd4107bbd3c",
   "metadata": {},
   "source": [
    "Assignment:\n",
    "1. In this notebook, we learned the various components of an LLM. \n",
    "    Your homework this week is to take the mini LLM we created from scratch and run your own training loop. Show how the training and validation perplexity change over the steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4b4a73-3f23-4ee7-8917-15bb902c1a4a",
   "metadata": {},
   "source": [
    "Copy/pasting the relevent code parts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbc6d6c5-6ea1-416e-bbc9-fe65736aa1aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1510029b5710>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4 ## so head_size = 16\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f74573c7-261a-4d2a-a404-1d896560df63",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset/input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dbe356d-bf9c-4ce1-81a2-db1f896fcdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C) 16,32,16\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd), # Projection layer going back into the residual pathway\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff014b4f-b3b3-46ac-913c-14fe289be125",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))    # Communication\n",
    "        x = x + self.ffwd(self.ln2(x))  # Computation\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b924084-92fd-4719-a0f1-11aee719c0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# super simple language model\n",
    "class LanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        self.to('cuda')\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5690936-22b8-4b45-8a77-4f618fba175b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c408426-170d-486d-9cee-c6abefe0422a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model):\n",
    "    out = {}\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "    #model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    #model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "525f97e3-b668-42ae-9817-33b11d3431f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_perplexity(loss):\n",
    "    out = {}\n",
    "    for label, tensor in loss.items():\n",
    "        perplexity = torch.exp(tensor)\n",
    "        out[label]=perplexity\n",
    "    print(f'perplexities: {out}')\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b33084-0586-4731-bb1d-03c99f4e3fca",
   "metadata": {},
   "source": [
    "Let's make a quick train model function that takes epochs as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b4d193f-e95c-4986-8d35-0698ac0b225a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epochs):\n",
    "    out = list()\n",
    "    for n in range(epochs):    \n",
    "        out.append(estimate_loss(model))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430dd043-80d4-4f34-b8cf-c6b588590d85",
   "metadata": {},
   "source": [
    "And make some parameter sets..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd5f323c-f215-4043-ae76-e7a36b7d3bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [(64,4,4),\n",
    "          (32,4,4), \n",
    "          (64,2,4), \n",
    "          (64,4,2),\n",
    "          (32,2,2),\n",
    "          (256,4,4)\n",
    "          ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da88d2b-8419-4c3b-a606-a9d671cb3b75",
   "metadata": {},
   "source": [
    "Running it out..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7abfed55-5ba2-40bd-8acc-9f9bd72d9036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_embd, n_head, n_layer = 64, 4, 4\n",
      "perplexities: {'train': tensor(77.9742), 'val': tensor(77.0933)}\n",
      "perplexities: {'train': tensor(77.4460), 'val': tensor(77.1539)}\n",
      "perplexities: {'train': tensor(77.6611), 'val': tensor(77.1097)}\n",
      "perplexities: {'train': tensor(77.8417), 'val': tensor(77.0538)}\n",
      "perplexities: {'train': tensor(77.7754), 'val': tensor(77.6035)}\n",
      "n_embd, n_head, n_layer = 32, 4, 4\n",
      "perplexities: {'train': tensor(75.1859), 'val': tensor(74.7931)}\n",
      "perplexities: {'train': tensor(75.4797), 'val': tensor(74.6318)}\n",
      "perplexities: {'train': tensor(75.2109), 'val': tensor(74.9816)}\n",
      "perplexities: {'train': tensor(75.3361), 'val': tensor(74.8588)}\n",
      "perplexities: {'train': tensor(75.4647), 'val': tensor(74.8104)}\n",
      "n_embd, n_head, n_layer = 64, 2, 4\n",
      "perplexities: {'train': tensor(81.7006), 'val': tensor(81.7795)}\n",
      "perplexities: {'train': tensor(81.4817), 'val': tensor(81.8034)}\n",
      "perplexities: {'train': tensor(81.6350), 'val': tensor(82.1424)}\n",
      "perplexities: {'train': tensor(81.5596), 'val': tensor(81.8983)}\n",
      "perplexities: {'train': tensor(81.8986), 'val': tensor(81.8517)}\n",
      "n_embd, n_head, n_layer = 64, 4, 2\n",
      "perplexities: {'train': tensor(79.6929), 'val': tensor(78.9958)}\n",
      "perplexities: {'train': tensor(79.7813), 'val': tensor(79.0896)}\n",
      "perplexities: {'train': tensor(79.7577), 'val': tensor(79.0481)}\n",
      "perplexities: {'train': tensor(79.8186), 'val': tensor(79.1945)}\n",
      "perplexities: {'train': tensor(79.6570), 'val': tensor(78.8772)}\n",
      "n_embd, n_head, n_layer = 32, 2, 2\n",
      "perplexities: {'train': tensor(78.7049), 'val': tensor(78.7361)}\n",
      "perplexities: {'train': tensor(78.7662), 'val': tensor(78.5350)}\n",
      "perplexities: {'train': tensor(78.7819), 'val': tensor(78.6891)}\n",
      "perplexities: {'train': tensor(78.6634), 'val': tensor(78.6764)}\n",
      "perplexities: {'train': tensor(78.6353), 'val': tensor(78.7404)}\n",
      "n_embd, n_head, n_layer = 256, 4, 4\n",
      "perplexities: {'train': tensor(76.0165), 'val': tensor(76.0303)}\n",
      "perplexities: {'train': tensor(75.8606), 'val': tensor(76.0504)}\n",
      "perplexities: {'train': tensor(75.9314), 'val': tensor(76.0765)}\n",
      "perplexities: {'train': tensor(75.8711), 'val': tensor(75.8961)}\n",
      "perplexities: {'train': tensor(75.8497), 'val': tensor(76.0655)}\n"
     ]
    }
   ],
   "source": [
    "outputs = {}\n",
    "for param in params:\n",
    "    n_embd, n_head, n_layer = param\n",
    "    label = f'n_embd, n_head, n_layer = {n_embd}, {n_head}, {n_layer}'\n",
    "    print(label)\n",
    "    model = LanguageModel()\n",
    "    losses=train_model(5)\n",
    "    perps = [find_perplexity(loss) for loss in losses]\n",
    "    outputs[label]=perps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363aec68-5b8b-4ce1-8648-f2d547a04099",
   "metadata": {},
   "source": [
    "It looks like the training didn't do much (possibly I didn't make it right. Changing the parameters definitely nudged the perplexities around, but it is clear that finding the effective set of parameters is a tricky task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience/conda-2023-10-04",
   "language": "python",
   "name": "conda-2023-10-03"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

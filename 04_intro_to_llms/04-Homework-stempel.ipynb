{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's write an elementary tokenizer that uses words as tokens.\n",
    "\n",
    "We will use Mark Twain's _Life On The Mississippi_ as a test bed. The text is in the accompanying file 'Life_On_The_Mississippi.txt'\n",
    "\n",
    "Here's a not-terribly-good such tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\ufeffThe', 1)\n",
      "('Project', 79)\n",
      "('Gutenberg', 22)\n",
      "('eBook', 4)\n",
      "('of', 4469)\n",
      "('Life', 5)\n",
      "('on', 856)\n",
      "('the', 8443)\n",
      "('Mississippi', 104)\n",
      "('This', 127)\n",
      "('ebook', 2)\n",
      "('is', 1076)\n",
      "('for', 1017)\n",
      "('use', 34)\n",
      "('anyone', 4)\n",
      "('anywhere', 8)\n",
      "('in', 2381)\n",
      "('United', 36)\n",
      "('States', 26)\n",
      "('and', 5692)\n",
      "('most', 119)\n",
      "('other', 223)\n",
      "('parts', 5)\n",
      "('world', 40)\n",
      "('at', 676)\n",
      "('no', 325)\n",
      "('cost', 18)\n",
      "('with', 1053)\n",
      "('almost', 37)\n",
      "('restrictions', 2)\n",
      "('whatsoever.', 2)\n",
      "('You', 92)\n",
      "('may', 85)\n",
      "('copy', 12)\n",
      "('it,', 199)\n",
      "('give', 67)\n",
      "('it', 1382)\n",
      "('away', 107)\n",
      "('or', 561)\n",
      "('re-use', 2)\n",
      "('under', 112)\n",
      "('terms', 22)\n",
      "('License', 8)\n",
      "('included', 2)\n",
      "('this', 591)\n",
      "('online', 4)\n",
      "('www.gutenberg.org.', 4)\n",
      "('If', 85)\n",
      "('you', 813)\n",
      "('are', 361)\n",
      "('not', 680)\n",
      "('located', 9)\n",
      "('States,', 8)\n",
      "('will', 287)\n",
      "('have', 557)\n",
      "('to', 3518)\n",
      "('check', 4)\n",
      "('laws', 13)\n",
      "('country', 50)\n",
      "('where', 152)\n",
      "('before', 150)\n",
      "('using', 10)\n",
      "('eBook.', 2)\n",
      "('Title:', 1)\n",
      "('Author:', 1)\n",
      "('Mark', 2)\n",
      "('Twain', 2)\n",
      "('Release', 1)\n",
      "('date:', 1)\n",
      "('July', 7)\n",
      "('10,', 2)\n",
      "('2004', 1)\n",
      "('[eBook', 1)\n",
      "('#245]', 1)\n",
      "('Most', 4)\n",
      "('recently', 3)\n",
      "('updated:', 1)\n",
      "('January', 2)\n",
      "('1,', 2)\n",
      "('2021', 1)\n",
      "('Language:', 1)\n",
      "('English', 7)\n",
      "('Credits:', 1)\n",
      "('Produced', 2)\n",
      "('by', 623)\n",
      "('David', 2)\n",
      "('Widger.', 2)\n",
      "('Earliest', 2)\n",
      "('PG', 3)\n",
      "('text', 4)\n",
      "('edition', 3)\n",
      "('produced', 15)\n",
      "('Graham', 2)\n",
      "('Allan', 2)\n",
      "('***', 4)\n",
      "('START', 1)\n",
      "('OF', 16)\n",
      "('THE', 29)\n",
      "('PROJECT', 4)\n",
      "('GUTENBERG', 3)\n"
     ]
    }
   ],
   "source": [
    "wdict = {}\n",
    "with open('Life_On_The_Mississippi.txt', 'r') as L:\n",
    "    line = L.readline()\n",
    "    nlines = 1\n",
    "    while line:\n",
    "\n",
    "        words = line.split()\n",
    "        for word in words:\n",
    "            if wdict.get(word) is not None:\n",
    "                wdict[word] += 1\n",
    "            else:\n",
    "                wdict[word] = 1\n",
    "        line = L.readline()\n",
    "        nlines += 1\n",
    "\n",
    "nitem = 0 ; maxitems = 100\n",
    "for item in wdict.items():\n",
    "    nitem += 1\n",
    "    print(item)\n",
    "    if nitem == maxitems: break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is unsatisfactory for a few reasons:\n",
    "\n",
    "* There are non-ASCII (Unicode) characters that should be stripped (the so-called \"Byte-Order Mark\" or BOM \\ufeff at the beginning of the text);\n",
    "\n",
    "* There are punctuation marks, which we don't want to concern ourselves with;\n",
    "\n",
    "* The same word can appear capitalized, or lower-case, or with its initial letter upper-cased, whereas we want them all to be normalized to lower-case.\n",
    "\n",
    "Part 1 of this assignment: insert code in this loop to operate on the str variable 'line' so as to fix these problems before 'line' is split into words.\n",
    "\n",
    "A hint to one possible way to do this: use the 'punctuation' character definition in the Python 'string' module, the 'maketrans' and 'translate' methods of Python's str class, to eliminate punctuation, and the regular expression ('re') Python module to eliminate any Unicode---it is useful to know that the regular expression r'[^\\x00-x7f]' means \"any character not in the vanilla ASCII set.\n",
    "\n",
    "Part 2: Add code to sort the contents of wdict by word occurrence frequency.  What are the top 100 most frequent word tokens?  Adding up occurrence frequencies starting from the most frequent words, how many distinct words make up the top 90% of word occurrences in this \"corpus\"?\n",
    "\n",
    "For this part, the docs of Python's 'sorted' and of the helper 'itemgetter' from 'operator' reward study.\n",
    "\n",
    "Write your modified code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 9366)\n",
      "('project', 90)\n",
      "('gutenberg', 96)\n",
      "('ebook', 13)\n",
      "('of', 4546)\n",
      "('life', 95)\n",
      "('on', 962)\n",
      "('mississippi', 169)\n",
      "('this', 795)\n",
      "('is', 1163)\n",
      "('for', 1123)\n",
      "('use', 51)\n",
      "('anyone', 5)\n",
      "('anywhere', 18)\n",
      "('in', 2622)\n",
      "('united', 37)\n",
      "('states', 52)\n",
      "('and', 6032)\n",
      "('most', 125)\n",
      "('other', 277)\n",
      "('parts', 9)\n",
      "('world', 75)\n",
      "('at', 755)\n",
      "('no', 447)\n",
      "('cost', 26)\n",
      "('with', 1096)\n",
      "('almost', 38)\n",
      "('restrictions', 2)\n",
      "('whatsoever', 2)\n",
      "('you', 1114)\n",
      "('may', 92)\n",
      "('copy', 17)\n",
      "('it', 2403)\n",
      "('give', 83)\n",
      "('away', 175)\n",
      "('or', 592)\n",
      "('re', 23)\n",
      "('under', 122)\n",
      "('terms', 27)\n",
      "('license', 27)\n",
      "('included', 3)\n",
      "('online', 4)\n",
      "('www', 9)\n",
      "('org', 9)\n",
      "('if', 384)\n",
      "('are', 389)\n",
      "('not', 735)\n",
      "('located', 9)\n",
      "('will', 303)\n",
      "('have', 572)\n",
      "('to', 3631)\n",
      "('check', 4)\n",
      "('laws', 20)\n",
      "('country', 77)\n",
      "('where', 180)\n",
      "('before', 214)\n",
      "('using', 11)\n",
      "('title', 3)\n",
      "('author', 3)\n",
      "('mark', 21)\n",
      "('twain', 28)\n",
      "('release', 1)\n",
      "('date', 18)\n",
      "('july', 7)\n",
      "('10', 11)\n",
      "('2004', 1)\n",
      "('245', 1)\n",
      "('recently', 4)\n",
      "('updated', 2)\n",
      "('january', 3)\n",
      "('1', 62)\n",
      "('2021', 1)\n",
      "('language', 13)\n",
      "('english', 13)\n",
      "('credits', 1)\n",
      "('produced', 22)\n",
      "('by', 744)\n",
      "('david', 2)\n",
      "('widger', 2)\n",
      "('earliest', 7)\n",
      "('pg', 3)\n",
      "('text', 4)\n",
      "('edition', 4)\n",
      "('graham', 2)\n",
      "('allan', 2)\n",
      "('start', 33)\n",
      "('table', 10)\n",
      "('contents', 6)\n",
      "('chapter', 125)\n",
      "('i', 2376)\n",
      "('well', 203)\n",
      "('worth', 37)\n",
      "('reading', 16)\n",
      "('about', 353)\n",
      "('remarkable', 14)\n",
      "('instead', 25)\n",
      "('widening', 2)\n",
      "('towards', 9)\n",
      "('its', 281)\n",
      "('mouth', 54)\n",
      "Number of distinct words: 12227\n"
     ]
    }
   ],
   "source": [
    "# part 1\n",
    "import string\n",
    "import re\n",
    "\n",
    "wdict = {}\n",
    "# from the hint, I'll make a maketrans for removing punctuation\n",
    "# this will replace punctuations with whitespaces to prevent some wonkiness\n",
    "punct_remover = str.maketrans(string.punctuation,' '*len(string.punctuation))\n",
    "with open('Life_On_The_Mississippi.txt', 'r') as L:\n",
    "    line = L.readline()\n",
    "    nlines = 1\n",
    "    while line:\n",
    "        # per the other hint, remove non-ASCII stuff using regex voodoo (yuck)\n",
    "        # note that the pattern is really r'[^\\x00-\\x7F]' (the notes are missing the second backslash)\n",
    "        line = re.sub(r'[^\\x00-\\x7F]', '', line)\n",
    "        line = line.translate(punct_remover)\n",
    "        \n",
    "        words = line.split()\n",
    "        for word in words:\n",
    "            # this will make it lowercase if it's not already\n",
    "            word = word.lower()\n",
    "            if wdict.get(word) is not None:\n",
    "                wdict[word] += 1\n",
    "            else:\n",
    "                wdict[word] = 1\n",
    "        line = L.readline()\n",
    "        nlines += 1\n",
    "\n",
    "nitem = 0 ; maxitems = 100\n",
    "for item in wdict.items():\n",
    "    nitem += 1\n",
    "    print(item)\n",
    "    if nitem == maxitems: break\n",
    "    \n",
    "print(f'Number of distinct words: {len(wdict)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not perfectâ€”contractions get split around the apostrophe and the website got split as well, but I'm satisfied.\n",
    "\n",
    "Now, for part 2.\n",
    "\n",
    "Since this is newer Python, dictionaries preserve order, so we can keep using it rather than convert it to a different data class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 9366)\n",
      "('and', 6032)\n",
      "('of', 4546)\n",
      "('a', 4235)\n",
      "('to', 3631)\n",
      "('in', 2622)\n",
      "('it', 2403)\n",
      "('i', 2376)\n",
      "('was', 2103)\n",
      "('that', 1789)\n",
      "('he', 1448)\n",
      "('is', 1163)\n",
      "('for', 1123)\n",
      "('you', 1114)\n",
      "('with', 1096)\n",
      "('but', 988)\n",
      "('his', 967)\n",
      "('on', 962)\n",
      "('had', 962)\n",
      "('as', 889)\n",
      "('this', 795)\n",
      "('they', 782)\n",
      "('at', 755)\n",
      "('by', 744)\n",
      "('all', 738)\n",
      "('not', 735)\n",
      "('one', 728)\n",
      "('s', 706)\n",
      "('there', 675)\n",
      "('were', 627)\n",
      "('be', 620)\n",
      "('or', 592)\n",
      "('my', 587)\n",
      "('from', 581)\n",
      "('have', 572)\n",
      "('so', 558)\n",
      "('out', 554)\n",
      "('we', 551)\n",
      "('up', 551)\n",
      "('me', 536)\n",
      "('him', 533)\n",
      "('when', 506)\n",
      "('river', 500)\n",
      "('which', 491)\n",
      "('would', 482)\n",
      "('t', 477)\n",
      "('an', 458)\n",
      "('no', 447)\n",
      "('them', 432)\n",
      "('then', 419)\n",
      "('said', 404)\n",
      "('are', 389)\n",
      "('if', 384)\n",
      "('now', 382)\n",
      "('their', 378)\n",
      "('time', 356)\n",
      "('about', 353)\n",
      "('down', 344)\n",
      "('been', 336)\n",
      "('could', 314)\n",
      "('what', 313)\n",
      "('has', 306)\n",
      "('two', 304)\n",
      "('will', 303)\n",
      "('into', 300)\n",
      "('man', 288)\n",
      "('her', 283)\n",
      "('its', 281)\n",
      "('other', 277)\n",
      "('some', 276)\n",
      "('do', 276)\n",
      "('new', 270)\n",
      "('she', 250)\n",
      "('water', 249)\n",
      "('any', 241)\n",
      "('more', 236)\n",
      "('boat', 235)\n",
      "('got', 235)\n",
      "('these', 234)\n",
      "('who', 232)\n",
      "('can', 230)\n",
      "('day', 225)\n",
      "('way', 220)\n",
      "('did', 217)\n",
      "('before', 214)\n",
      "('here', 211)\n",
      "('over', 207)\n",
      "('hundred', 205)\n",
      "('well', 203)\n",
      "('old', 200)\n",
      "('upon', 200)\n",
      "('after', 198)\n",
      "('pilot', 197)\n",
      "('good', 195)\n",
      "('through', 194)\n",
      "('than', 193)\n",
      "('get', 193)\n",
      "('every', 187)\n",
      "('never', 185)\n",
      "('went', 184)\n"
     ]
    }
   ],
   "source": [
    "# part 2: finding the top 100 most-used words\n",
    "sorted_wdict = dict(sorted(wdict.items(), key = lambda item: item[1], reverse = True))\n",
    "                           \n",
    "nitem = 0 ; maxitems = 100\n",
    "for item in sorted_wdict.items():\n",
    "    nitem += 1\n",
    "    print(item)\n",
    "    if nitem == maxitems: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks reasonable. Now, some basic manipulation to figure out how many distinct words make up 90% of occurances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words: 152328. 90% cutoff: 137095.2\n",
      "Displaying top five most used words:\n",
      "[the]: count=9366, or 6.15% of total word count.\n",
      "[and]: count=6032, or 3.96% of total word count.\n",
      "[of]: count=4546, or 2.98% of total word count.\n",
      "[a]: count=4235, or 2.78% of total word count.\n",
      "[to]: count=3631, or 2.38% of total word count.\n",
      "Distinct words making up the top 90%: 3079, out of 12227 total distinct words\n"
     ]
    }
   ],
   "source": [
    "nwords = sum(sorted_wdict.values())\n",
    "nwords_top90 = 0.9*nwords\n",
    "print(f'total words: {nwords}. 90% cutoff: {nwords_top90}')\n",
    "\n",
    "words_accumulated = 0\n",
    "distinct_words = 0\n",
    "print('Displaying top five most used words:')\n",
    "for word,count in sorted_wdict.items():\n",
    "    words_accumulated += count\n",
    "    distinct_words += 1\n",
    "    # let's print out the first five\n",
    "    if distinct_words < 6:\n",
    "        fraction_nwords = count/nwords\n",
    "        print(f'[{word}]: count={count}, or {fraction_nwords:.2%} of total word count.')\n",
    "    if words_accumulated > nwords_top90: break\n",
    "    \n",
    "print(f'Distinct words making up the top 90%: {distinct_words}, out of {len(wdict)} total distinct words')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience/conda-2023-10-04",
   "language": "python",
   "name": "conda-2023-10-03"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5faee057-41e4-425a-83bb-2f6c652facb2",
   "metadata": {},
   "source": [
    "Sam Tempel's homework for session 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f42104-b06c-47b0-aab1-818606d60017",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Homework: train a Nonlinear Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c924d4c-ed2a-496a-9d2e-92121b20cdd7",
   "metadata": {},
   "source": [
    "1. Write some code to train the NonlinearClassifier.\n",
    "2. Create a data loader for the test data and check your model's accuracy on the test data. \n",
    "\n",
    "If you have time, experiment with how to improve the model. Note: training and validation data can be used to compare models, but test data should be saved until the end as a final check of generalization. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb307391-1a22-41da-81fc-274551ac0e28",
   "metadata": {},
   "source": [
    "I'm still learning Jupyter notebooks, so apologies if this is weirdly formatted.\n",
    "\n",
    "First, I'm going to copy/paste the needed code from the lecture notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3ea3449-fa02-4ca5-9d3d-98e9d635d518",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "\n",
    "import numpy \n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9744cc98-ee77-4ca2-bb92-c90b90dc1636",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = torchvision.datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.ToTensor()\n",
    ")\n",
    "\n",
    "test_data = torchvision.datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.ToTensor()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9679fd24-edf3-4e12-a33e-efa2547bd5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST data loaded: train: 48000  examples, validation:  12000 examples, test: 10000 examples\n",
      "Input shape torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "training_data, validation_data = torch.utils.data.random_split(training_data, [0.8, 0.2], generator=torch.Generator().manual_seed(55))\n",
    "print('MNIST data loaded: train:',len(training_data),' examples, validation: ', len(validation_data), 'examples, test:',len(test_data), 'examples')\n",
    "print('Input shape', training_data[0][0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac1040ab-1844-4efa-a4a8-f759192a69ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# The dataloader makes our dataset iterable \n",
    "train_dataloader128 = torch.utils.data.DataLoader(training_data, batch_size=batch_size)\n",
    "val_dataloader128 = torch.utils.data.DataLoader(validation_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24356915-27cf-4592-b44f-ed69ef2f90e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "# The dataloader makes our dataset iterable \n",
    "train_dataloader256 = torch.utils.data.DataLoader(training_data, batch_size=batch_size)\n",
    "val_dataloader256 = torch.utils.data.DataLoader(validation_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4faead0b-afe3-444d-869f-8ca739595fd6",
   "metadata": {},
   "source": [
    "Alright, this time I'm using the nonlinear model as defined in the lecture notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "118a016c-bfda-49ae-b34a-79a9410bb84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonlinearClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.layers_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 50),\n",
    "            nn.ReLU(),\n",
    "            ##nn.Dropout(0.2),\n",
    "            nn.Linear(50, 50),\n",
    "            nn.ReLU(),\n",
    "            ##nn.Dropout(0.2),\n",
    "            nn.Linear(50, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.layers_stack(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1da012-d984-4648-85e5-c9e5d3228bf2",
   "metadata": {},
   "source": [
    "I'm going to create two nonlinear model objects, one for each batch size. I'll leave learning rate alone for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e383d5fb-31f8-49b3-9260-e190519ddfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "nonlinear_model128 = NonlinearClassifier()\n",
    "nonlinear_model256 = NonlinearClassifier()\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer128 = torch.optim.SGD(nonlinear_model128.parameters(), lr=0.01)\n",
    "optimizer256 = torch.optim.SGD(nonlinear_model256.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5decd708-0eb1-4a3f-94a4-c50365aae97b",
   "metadata": {},
   "source": [
    "Looks alright. It seems I can recycle the training and evaluate function, and try going through some training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90d3f3ea-b81f-401c-891b-daa3b95d8fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # forward pass\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # backward pass calculates gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # take one step with these gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        # resets the gradients \n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41ed7116-62e0-450c-9cdb-902d080c968b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - some NN pieces behave differently during training\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    loss, correct = 0, 0\n",
    "\n",
    "    # We can save computation and memory by not calculating gradients here - we aren't optimizing \n",
    "    with torch.no_grad():\n",
    "        # loop over all of the batches\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            loss += loss_fn(pred, y).item()\n",
    "            # how many are correct in this batch? Tracking for accuracy \n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    loss /= num_batches\n",
    "    correct /= size\n",
    "    \n",
    "    accuracy = 100*correct\n",
    "    return accuracy, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c8ef10-7d49-4eaa-8723-9d3e64ade16e",
   "metadata": {},
   "source": [
    "Alright, let's run the one with batch_size=128 through 10 epochs and see what happens.\n",
    "\n",
    "Let's also define a function for the training a number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68ba0c65-4a37-44ed-a385-e6da8b7d9802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: training loss: 2.054208186149597, accuracy: 43.4\n",
      "Epoch 1: training loss: 1.1352396494547525, accuracy: 75.03958333333334\n",
      "Epoch 2: training loss: 0.6668230414390564, accuracy: 82.23125\n",
      "Epoch 3: training loss: 0.5226160089969635, accuracy: 85.6625\n",
      "Epoch 4: training loss: 0.4524427552223206, accuracy: 87.60208333333334\n",
      "Epoch 5: training loss: 0.4110342762072881, accuracy: 88.60625\n",
      "Epoch 6: training loss: 0.3838413459857305, accuracy: 89.3375\n",
      "Epoch 7: training loss: 0.3642228608528773, accuracy: 89.89583333333333\n",
      "Epoch 8: training loss: 0.3488120183547338, accuracy: 90.31041666666667\n",
      "Epoch 9: training loss: 0.33597377097606657, accuracy: 90.62291666666667\n",
      "CPU times: user 36min 15s, sys: 45.2 s, total: 37min\n",
      "Wall time: 1min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def training_session(epochs, dataloader, model, loss_fn, optimizer):\n",
    "    for j in range(epochs):\n",
    "        train_one_epoch(dataloader, model, loss_fn, optimizer)\n",
    "\n",
    "        # checking on the training loss and accuracy once per epoch\n",
    "        acc, loss = evaluate(dataloader, model, loss_fn)\n",
    "        print(f\"Epoch {j}: training loss: {loss}, accuracy: {acc}\")\n",
    "\n",
    "epochs = 10\n",
    "training_session(epochs, train_dataloader128, nonlinear_model128, loss_fn, optimizer128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d486d210-498e-4f7c-8e26-8a84eca1eca5",
   "metadata": {},
   "source": [
    "Same thing for the one with batch_size=256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bb000c5-d103-4959-bbab-9eabe426f4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: training loss: 2.240848493068776, accuracy: 25.96875\n",
      "Epoch 1: training loss: 2.0775381846630827, accuracy: 37.329166666666666\n",
      "Epoch 2: training loss: 1.7491438699529527, accuracy: 51.4375\n",
      "Epoch 3: training loss: 1.3178486551376098, accuracy: 70.375\n",
      "Epoch 4: training loss: 0.9613915868896119, accuracy: 77.85416666666667\n",
      "Epoch 5: training loss: 0.7571233054424854, accuracy: 81.19375\n",
      "Epoch 6: training loss: 0.6360275720028167, accuracy: 83.50416666666666\n",
      "Epoch 7: training loss: 0.5559374755050274, accuracy: 85.12916666666666\n",
      "Epoch 8: training loss: 0.5009567452237961, accuracy: 86.40833333333333\n",
      "Epoch 9: training loss: 0.4625019635608856, accuracy: 87.33333333333333\n",
      "CPU times: user 35min 57s, sys: 43.8 s, total: 36min 41s\n",
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "training_session(epochs, train_dataloader256, nonlinear_model256, loss_fn, optimizer256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd839719-eae4-49ee-b11b-925ae55745f8",
   "metadata": {},
   "source": [
    "Seems to have no effect on speed and some change in accuracy. Let's also try the nonlinear classifier with dropouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "391ad3d0-7dc7-4f74-a8e2-3d2ffc8f4509",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NonlinearClassifierWithDrops(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.layers_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(50, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(50, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.layers_stack(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6477ae18-3bf3-437a-a0aa-d97251baa476",
   "metadata": {},
   "outputs": [],
   "source": [
    "droppy_model128 = NonlinearClassifierWithDrops()\n",
    "droppy_model256 = NonlinearClassifierWithDrops()\n",
    "\n",
    "optimizerD128 = torch.optim.SGD(droppy_model128.parameters(), lr=0.01)\n",
    "optimizerD256 = torch.optim.SGD(droppy_model256.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8485f12c-2485-4099-9fe8-4805ee723006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: training loss: 2.162392141342163, accuracy: 43.51875\n",
      "Epoch 1: training loss: 1.467673095703125, accuracy: 66.62291666666667\n",
      "Epoch 2: training loss: 0.9030340145428976, accuracy: 78.1375\n",
      "Epoch 3: training loss: 0.6756682435671488, accuracy: 83.25625\n",
      "Epoch 4: training loss: 0.5637385493119558, accuracy: 85.51875\n",
      "Epoch 5: training loss: 0.4956769909063975, accuracy: 86.85833333333333\n",
      "Epoch 6: training loss: 0.45151536536216735, accuracy: 87.89375\n",
      "Epoch 7: training loss: 0.4186584941546122, accuracy: 88.59166666666667\n",
      "Epoch 8: training loss: 0.39290662050247194, accuracy: 89.17291666666667\n",
      "Epoch 9: training loss: 0.37290904625256854, accuracy: 89.62708333333333\n",
      "CPU times: user 36min 34s, sys: 45.4 s, total: 37min 20s\n",
      "Wall time: 1min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "training_session(epochs, train_dataloader128, droppy_model128, loss_fn, optimizerD128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c48af8-0f5f-4ff0-9d85-1ec8e6f3f84f",
   "metadata": {},
   "source": [
    "And of course the larger batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da8024ab-f344-468c-ab5f-1c38544b06ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: training loss: 2.243618130683899, accuracy: 26.49375\n",
      "Epoch 1: training loss: 2.105739229537071, accuracy: 44.875\n",
      "Epoch 2: training loss: 1.7990784860671836, accuracy: 56.574999999999996\n",
      "Epoch 3: training loss: 1.400007629648168, accuracy: 68.34791666666666\n",
      "Epoch 4: training loss: 1.0643982535347025, accuracy: 75.4875\n",
      "Epoch 5: training loss: 0.8561332070447029, accuracy: 79.86666666666666\n",
      "Epoch 6: training loss: 0.730108638710164, accuracy: 82.22708333333333\n",
      "Epoch 7: training loss: 0.6462055064262228, accuracy: 83.69583333333334\n",
      "Epoch 8: training loss: 0.5895016921010423, accuracy: 84.88333333333333\n",
      "Epoch 9: training loss: 0.5472904966866716, accuracy: 85.68124999999999\n",
      "CPU times: user 36min 28s, sys: 46.2 s, total: 37min 14s\n",
      "Wall time: 1min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "training_session(epochs, train_dataloader256, droppy_model256, loss_fn, optimizerD256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86813d80-e169-43d2-a245-0e1f9a037247",
   "metadata": {},
   "source": [
    "Let's check validity for the first version (without dropouts), batch_size=128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6892543-47fb-47cf-840d-3e684bd77737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.3242, validation accuracy: 90.58%\n"
     ]
    }
   ],
   "source": [
    "acc_val, loss_val = evaluate(val_dataloader128, nonlinear_model128, loss_fn)\n",
    "print(\"Validation loss: %.4f, validation accuracy: %.2f%%\" % (loss_val, acc_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbc68be-de61-48c6-b662-b77145da75bc",
   "metadata": {},
   "source": [
    "And the same for the batch_size=256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d3ab9b6-bcb5-4ee7-9dc7-b2438ecf3279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.4558, validation accuracy: 87.35%\n"
     ]
    }
   ],
   "source": [
    "acc_val, loss_val = evaluate(val_dataloader256, nonlinear_model256, loss_fn)\n",
    "print(\"Validation loss: %.4f, validation accuracy: %.2f%%\" % (loss_val, acc_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6bc07d-c148-4292-af6f-5a1b868d8593",
   "metadata": {},
   "source": [
    "Now droppy version, batch_size=128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44ee0e33-9827-42a9-bd0e-e4c71c42fdae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.3658, validation accuracy: 89.54%\n"
     ]
    }
   ],
   "source": [
    "acc_val, loss_val = evaluate(val_dataloader128, droppy_model128, loss_fn)\n",
    "print(\"Validation loss: %.4f, validation accuracy: %.2f%%\" % (loss_val, acc_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e127457-c597-47a8-8c9d-64e9bd34d7a2",
   "metadata": {},
   "source": [
    "And droppy version, batch_size=256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2464d091-9674-464c-a2f1-ebdfe7d26456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.5437, validation accuracy: 85.64%\n"
     ]
    }
   ],
   "source": [
    "acc_val, loss_val = evaluate(val_dataloader256, droppy_model256, loss_fn)\n",
    "print(\"Validation loss: %.4f, validation accuracy: %.2f%%\" % (loss_val, acc_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab742c0-4377-4fbd-a0f2-e161efa54e19",
   "metadata": {},
   "source": [
    "Cool. Definitely more to explore by changing learning rate and fiddling with epoch count, but I'll call it quits here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience/conda-2023-01-10",
   "language": "python",
   "name": "conda-2023-01-10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
